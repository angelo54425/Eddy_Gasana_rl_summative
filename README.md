# ğŸ“ Career Path Reinforcement Learning Environment  
*A custom RL environment simulating a student's journey through different career fields.*

---

## ğŸ“Œ Overview  
This project implements a fully custom **Reinforcement Learning environment** where an agent represents a student navigating toward a successful career path. The grid-world environment includes four career zones â€” **Private, Medical, Finance, Engineering** â€” each with training tiles and a final goal (star). The agent must choose a career, train sufficiently, and reach its goal with enough skill.

The project includes:

- A complete **Gymnasium custom environment**  
- A **Pygame renderer** for visualization  
- Training scripts for **DQN, PPO, A2C, and REINFORCE**  
- Evaluation utilities & metrics  
- Result visualizations (reward curves)  
- A runnable agent demo (`main.py`)

---

## ğŸ—‚ Project Structure


---

## ğŸ§  Environment Description

### ğŸŸ¦ Grid Layout  
- A **10Ã—10 grid**  
- **4 career zones** (colored)  
- A central neutral zone  
- Training tiles specific to each career  
- A final star goal per career  
- Once a zone is entered â†’ it becomes **locked**

---

### ğŸ® Action Space â€” `Discrete(6)`

| Action | Meaning     |
|--------|-------------|
| 0 | Up    |
| 1 | Down  |
| 2 | Left  |
| 3 | Right |
| 4 | Wait  |
| 5 | Train |

---

### ğŸ‘ Observation Space (`shape = (5,)`)

---

## ğŸ† Reward Structure

- **âˆ’0.05** per step  
- **âˆ’0.02** idle penalty  
- **âˆ’0.20** invalid movement  
- **+1.0** when entering chosen career zone  
- **+0.05 â†’ +3** for training (scaled)  
- **Small reward** for touching star early (episode continues)  
- **+80** reaching the final star with enough skill  
- **âˆ’20** timeout penalty  

This encourages exploration, focused training, and efficient reaching of the career goal.

---

## ğŸ¤– Algorithms Implemented

### **Deep Q-Network (DQN)**  
- Replay memory  
- Target network  
- Îµ-greedy exploration  
- Tuned for sparse rewards  
- **Best-performing model**

### **PPO (Proximal Policy Optimization)**  
- Clipped objective  
- GAE  
- Stable Actor-Critic architecture

### **A2C (Advantage Actor Critic)**  
- Shared network  
- Entropy regularization  

### **REINFORCE (Monte-Carlo Policy Gradient)**  
- Baseline-free  
- Fully stochastic policy  
- Struggles with sparse reward design  

---

## ğŸ“ˆ Evaluation Results

| Algorithm | Mean Reward | Success Full | Partial | Failure |
|----------|-------------|--------------|---------|---------|
| **DQN** | 33.5 | **60%** | 20% | 20% |
| **A2C** | 3.64 | 4% | 16% | 80% |
| **PPO** | -427 | 0% | 0% | 100% |
| **REINFORCE** | -417 | 0% | 0% | 100% |

â¡ **DQN is the clear top performer**

---

## â–¶ Run Demo (Random Agent)
```bash
python3 tests/test_pygame.py

Let me know if you want:

âœ” Shields.io badges  
âœ” A GIF of the environment  
âœ” A â€œFuture Workâ€ section  
âœ” A contributors section  

I can add any of these!
